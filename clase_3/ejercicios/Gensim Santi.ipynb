{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Custom embedddings con Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA7nqkumo9z9"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto. Se utilizará canciones de bandas para generar los embeddings, es decir, que los vectores tendrán la forma en función de como esa banda haya utilizado las palabras en sus canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset canciones de bandas de habla inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\santi/gensim-data\\\\fake-news\\\\fake-news.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path=api.load(\"fake-news\",return_path=True)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\santi\\\\\\\\gensim-data\\\\\\\\fake-news\\\\\\\\fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\clase_3\\ejercicios\\Gensim Santi.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/ejercicios/Gensim%20Santi.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39msanti\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mgensim-data\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mfake-news\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mfake\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/ejercicios/Gensim%20Santi.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\\\\\Users\\\\\\\\santi\\\\\\\\gensim-data\\\\\\\\fake-news\\\\\\\\fake'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\\\Users\\\\santi\\\\gensim-data\\\\fake-news\\\\fake', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l7z4CSBfpR3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset ya se encuentra descargado\n"
     ]
    }
   ],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "import os\n",
    "import platform\n",
    "if os.access('./songs_dataset', os.F_OK) is False:\n",
    "    if os.access('songs_dataset.zip', os.F_OK) is False:\n",
    "        if platform.system() == 'Windows':\n",
    "            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n",
    "        else:\n",
    "            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n",
    "    !unzip -q songs_dataset.zip   \n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mysGrIw9ljC2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adele.txt',\n",
       " 'al-green.txt',\n",
       " 'alicia-keys.txt',\n",
       " 'amy-winehouse.txt',\n",
       " 'beatles.txt',\n",
       " 'bieber.txt',\n",
       " 'bjork.txt',\n",
       " 'blink-182.txt',\n",
       " 'bob-dylan.txt',\n",
       " 'bob-marley.txt',\n",
       " 'britney-spears.txt',\n",
       " 'bruce-springsteen.txt',\n",
       " 'bruno-mars.txt',\n",
       " 'cake.txt',\n",
       " 'dickinson.txt',\n",
       " 'disney.txt',\n",
       " 'dj-khaled.txt',\n",
       " 'dolly-parton.txt',\n",
       " 'dr-seuss.txt',\n",
       " 'drake.txt',\n",
       " 'eminem.txt',\n",
       " 'janisjoplin.txt',\n",
       " 'jimi-hendrix.txt',\n",
       " 'johnny-cash.txt',\n",
       " 'joni-mitchell.txt',\n",
       " 'kanye-west.txt',\n",
       " 'kanye.txt',\n",
       " 'Kanye_West.txt',\n",
       " 'lady-gaga.txt',\n",
       " 'leonard-cohen.txt',\n",
       " 'lil-wayne.txt',\n",
       " 'Lil_Wayne.txt',\n",
       " 'lin-manuel-miranda.txt',\n",
       " 'lorde.txt',\n",
       " 'ludacris.txt',\n",
       " 'michael-jackson.txt',\n",
       " 'missy-elliott.txt',\n",
       " 'nickelback.txt',\n",
       " 'nicki-minaj.txt',\n",
       " 'nirvana.txt',\n",
       " 'notorious-big.txt',\n",
       " 'notorious_big.txt',\n",
       " 'nursery_rhymes.txt',\n",
       " 'patti-smith.txt',\n",
       " 'paul-simon.txt',\n",
       " 'prince.txt',\n",
       " 'r-kelly.txt',\n",
       " 'radiohead.txt',\n",
       " 'rihanna.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posibles bandas\n",
    "os.listdir(\"./songs_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_1420\\3849064916.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yesterday, all my troubles seemed so far away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now it looks as though they're here to stay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh, I believe in yesterday Suddenly, I'm not h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There's a shadow hanging over me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, yesterday came suddenly Why she had to go ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0      Yesterday, all my troubles seemed so far away\n",
       "1        Now it looks as though they're here to stay\n",
       "2  Oh, I believe in yesterday Suddenly, I'm not h...\n",
       "3                  There's a shadow hanging over me.\n",
       "4  Oh, yesterday came suddenly Why she had to go ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n",
    "df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de documentos: 1846\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de documentos:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "### 1 - Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
    "for _, row in df[:None].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['yesterday', 'all', 'my', 'troubles', 'seemed', 'so', 'far', 'away'],\n",
       " ['now', 'it', 'looks', 'as', 'though', \"they're\", 'here', 'to', 'stay']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demos un vistazo\n",
    "sentence_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "### 2 - Crear los vectores (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i0wnDdv9sJ47"
   },
   "outputs": [],
   "source": [
    "# Crearmos el modelo generador de vectores\n",
    "# En este caso utilizaremos la estructura modelo Skipgram\n",
    "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5lTt8wErsf17"
   },
   "outputs": [],
   "source": [
    "# Obtener el vocabulario con los tokens\n",
    "w2v_model.build_vocab(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TNc9qt4os5AT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 1846\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de filas/docs encontradas en el corpus\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "idw9cHF3tSMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de words distintas en el corpus: 445\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de words encontradas en el corpus\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC9mZ8DPk-UC"
   },
   "source": [
    "### 3 - Entrenar el modelo generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QSp-x0PAsq56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 113208.15625\n",
      "Loss after epoch 1: 65953.953125\n",
      "Loss after epoch 2: 65934.234375\n",
      "Loss after epoch 3: 65712.0625\n",
      "Loss after epoch 4: 63872.53125\n",
      "Loss after epoch 5: 64142.3125\n",
      "Loss after epoch 6: 64058.9375\n",
      "Loss after epoch 7: 64755.375\n",
      "Loss after epoch 8: 62585.375\n",
      "Loss after epoch 9: 60415.1875\n",
      "Loss after epoch 10: 59823.6875\n",
      "Loss after epoch 11: 58930.1875\n",
      "Loss after epoch 12: 57726.1875\n",
      "Loss after epoch 13: 56493.125\n",
      "Loss after epoch 14: 55842.5\n",
      "Loss after epoch 15: 55862.5625\n",
      "Loss after epoch 16: 51696.0\n",
      "Loss after epoch 17: 49815.25\n",
      "Loss after epoch 18: 49568.125\n",
      "Loss after epoch 19: 48959.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156986, 287740)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_model.train(sentence_tokens,\n",
    "                 total_examples=w2v_model.corpus_count,\n",
    "                 epochs=20,\n",
    "                 compute_loss = True,\n",
    "                 callbacks=[callback()]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### 4 - Ensayar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pretty', 0.908713161945343),\n",
       " ('sleep', 0.8649675250053406),\n",
       " ('help', 0.8594343662261963),\n",
       " ('try', 0.845602810382843),\n",
       " ('cry', 0.8387219905853271),\n",
       " ('little', 0.8273669481277466),\n",
       " ('not', 0.8181101679801941),\n",
       " ('seems', 0.8175199627876282),\n",
       " ('twist', 0.8154971599578857),\n",
       " ('peace', 0.8122653961181641)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"darling\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "47HiU5gdkdMq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shake', -0.21786823868751526),\n",
       " ('our', -0.24495825171470642),\n",
       " ('come', -0.2613290548324585),\n",
       " ('on', -0.26722246408462524),\n",
       " ('bang', -0.2732936441898346),\n",
       " ('five', -0.2741844654083252),\n",
       " ('six', -0.2774016261100769),\n",
       " ('baby', -0.2776325047016144),\n",
       " ('work', -0.27903106808662415),\n",
       " ('four', -0.27927592396736145)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MENOS se relacionan con...:\n",
    "w2v_model.wv.most_similar(negative=[\"love\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DT4Rvno2mD65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('five', 0.9786689877510071),\n",
       " ('three', 0.9761053323745728),\n",
       " ('six', 0.9668906331062317),\n",
       " ('two', 0.9565715193748474),\n",
       " ('seven', 0.9542327523231506),\n",
       " ('sixty', 0.9003345370292664),\n",
       " ('one', 0.8165066242218018),\n",
       " ('us', 0.7762900590896606),\n",
       " ('crying', 0.7727378010749817),\n",
       " ('strawberry', 0.7672456502914429)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"four\"], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XPLDPgzBmQXt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"can't\", 0.9409608840942383),\n",
       " ('buy', 0.939951479434967),\n",
       " ('much', 0.8996829390525818),\n",
       " ('hide', 0.8465200066566467),\n",
       " ('just', 0.8415135145187378)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras que MÁS se relacionan con...:\n",
    "w2v_model.wv.most_similar(positive=[\"money\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "L_UvHPMMklOr"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'diedaa' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\clase_3\\jupyter_notebooks\\3b - Custom embedding con Gensim.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/jupyter_notebooks/3b%20-%20Custom%20embedding%20con%20Gensim.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Ensayar con una palabra que no está en el vocabulario:\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/jupyter_notebooks/3b%20-%20Custom%20embedding%20con%20Gensim.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m w2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mmost_similar(negative\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mdiedaa\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'diedaa' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "# Ensayar con una palabra que no está en el vocabulario:\n",
    "w2v_model.wv.most_similar(negative=[\"diedaa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### 5 - Visualizar agrupación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  \n",
    "\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\clase_3\\jupyter_notebooks\\3b - Custom embedding con Gensim.ipynb Cell 28\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/jupyter_notebooks/3b%20-%20Custom%20embedding%20con%20Gensim.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m MAX_WORDS\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/jupyter_notebooks/3b%20-%20Custom%20embedding%20con%20Gensim.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mscatter(x\u001b[39m=\u001b[39mx_vals[:MAX_WORDS], y\u001b[39m=\u001b[39my_vals[:MAX_WORDS], text\u001b[39m=\u001b[39mlabels[:MAX_WORDS])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_3/jupyter_notebooks/3b%20-%20Custom%20embedding%20con%20Gensim.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m fig\u001b[39m.\u001b[39;49mshow(renderer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcolab\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# esto para plotly en colab\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\basedatatypes.py:3409\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3377\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[0;32m   3378\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3405\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3407\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m-> 3409\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\plotly\\io\\_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    393\u001b[0m         )\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(w2v_model)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=x_vals[:MAX_WORDS], y=y_vals[:MAX_WORDS], text=labels[:MAX_WORDS])\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMM_SHSaZ9N-"
   },
   "source": [
    "### Alumno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WivQZ3ZCZ9N_"
   },
   "source": [
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "- Graficarlos.\n",
    "- Obtener conclusiones."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
