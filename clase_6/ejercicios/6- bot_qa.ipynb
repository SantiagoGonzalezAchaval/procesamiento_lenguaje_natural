{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM Bot QA"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\\\n","[LINK](http://convai.io/data/)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import one_hot\n","from tensorflow.keras.utils import pad_sequences\n","from keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Activation, Dropout, Dense, Flatten, LSTM, SimpleRNN\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"RHNkUaPp6aYq"},"outputs":[{"name":"stdout","output_type":"stream","text":["El dataset ya se encuentra descargado\n"]}],"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('data_volunteers.json', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n","    output = 'data_volunteers.json'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"WZy1-wgG-Rp7"},"outputs":[],"source":["# dataset_file\n","import json\n","\n","text_file = \"data_volunteers.json\"\n","with open(text_file) as f:\n","    data = json.load(f) # la variable data será un diccionario\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"ue5qd54S-eew"},"outputs":[{"data":{"text/plain":["dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Observar los campos disponibles en cada linea del dataset\n","data[0].keys()"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"jHBRAXPl-3dz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cantidad de rows utilizadas: 6033\n"]}],"source":["chat_in = []\n","chat_out = []\n","\n","input_sentences = []\n","output_sentences = []\n","output_sentences_inputs = []\n","max_len = 30\n","\n","def clean_text(txt):\n","    txt = txt.lower()    \n","    txt.replace(\"\\'d\", \" had\")\n","    txt.replace(\"\\'s\", \" is\")\n","    txt.replace(\"\\'m\", \" am\")\n","    txt.replace(\"don't\", \"do not\")\n","    txt = re.sub(r'\\W+', ' ', txt)\n","    \n","    return txt\n","\n","for line in data:\n","    for i in range(len(line['dialog'])-1):\n","        # vamos separando el texto en \"preguntas\" (chat_in)\n","        # y \"respuestas\" (chat_out)\n","        chat_in = clean_text(line['dialog'][i]['text'])\n","        chat_out = clean_text(line['dialog'][i+1]['text'])\n","\n","        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n","            continue\n","\n","        input_sentence, output = chat_in, chat_out\n","        \n","        # output sentence (decoder_output) tiene <eos>\n","        output_sentence = output + ' <eos>'\n","        # output sentence input (decoder_input) tiene <sos>\n","        output_sentence_input = '<sos> ' + output\n","\n","        input_sentences.append(input_sentence)\n","        output_sentences.append(output_sentence)\n","        output_sentences_inputs.append(output_sentence_input)\n","\n","print(\"Cantidad de rows utilizadas:\", len(input_sentences))"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"07L1qj8pC_l6"},"outputs":[{"data":{"text/plain":["('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["input_sentences[1], output_sentences[1], output_sentences_inputs[1]"]},{"cell_type":"markdown","metadata":{"id":"8P-ynUNP5xp6"},"source":["### 2 - Preprocesamiento\n","Realizar el preprocesamiento necesario para obtener:\n","- word2idx_inputs, max_input_len\n","- word2idx_outputs, max_out_len, num_words_output\n","- encoder_input_sequences, decoder_output_sequences, decoder_targets"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Palabras en el vocabulario: 1799\n","Sentencia de entrada más larga: 9\n"]}],"source":["# Tokenizar las palabras con el Tokenizer de Keras\n","# Definir una máxima cantidad de palabras a utilizar:\n","# - num_words --> the maximum number of words to keep, based on word frequency.\n","# - Only the most common num_words-1 words will be kept.\n","from keras.preprocessing.text import Tokenizer\n","\n","# tokenizador de inglés\n","input_tokenizer = Tokenizer()\n","input_tokenizer.fit_on_texts(input_sentences)\n","input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n","\n","word2idx_inputs = input_tokenizer.word_index\n","print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n","\n","max_input_len = max(len(sen) for sen in input_integer_seq)\n","print(\"Sentencia de entrada más larga:\", max_input_len)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Palabras en el vocabulario: 1806\n","Sentencia de salida más larga: 10\n"]}],"source":["# sacamos los \"<>\" para que no afectar nuestros tokens\n","output_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n","output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n","output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n","\n","word2idx_outputs = output_tokenizer.word_index\n","print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n","\n","num_words_output = len(word2idx_outputs) + 1\n","# Se suma 1 para incluir el token de palabra desconocida\n","\n","max_out_len = max(len(sen) for sen in output_integer_seq)\n","print(\"Sentencia de salida más larga:\", max_out_len)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cantidad de rows del dataset: 6033\n","encoder_input_sequences shape: (6033, 9)\n","decoder_input_sequences shape: (6033, 10)\n"]}],"source":["print(\"Cantidad de rows del dataset:\", len(input_integer_seq))\n","\n","encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n","print(\"encoder_input_sequences shape:\", encoder_input_sequences.shape)\n","\n","decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n","print(\"decoder_input_sequences shape:\", decoder_input_sequences.shape)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["(6033, 10, 1807)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["from keras.utils import to_categorical\n","decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n","decoder_targets = to_categorical(decoder_output_sequences, num_classes=num_words_output)\n","decoder_targets.shape"]},{"cell_type":"markdown","metadata":{"id":"_CJIsLBbj6rg"},"source":["### 3 - Preparar los embeddings\n","Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Los embeddings gloveembedding.pkl ya están descargados\n"]}],"source":["import os\n","import gdown\n","if os.access('gloveembedding.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KY6avD5I1eI2dxQzMkR3WExwKwRq2g94&export=download'\n","    output = 'gloveembedding.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dicctionarios para traducir de embedding a IDX de la palabra\n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    WORD_MAX_SIZE = 60\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["#Uso glove que tiene muchos menos features para poder correrlo.\n","model_embeddings = GloveEmbeddings()"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["preparing embedding matrix...\n","number of null word embeddings: 38\n","Shape de los embeddings:  (1799, 50)\n"]}],"source":["print('preparing embedding matrix...')\n","embed_dim = model_embeddings.N_FEATURES\n","words_not_found = []\n","\n","# word_index provieen del tokenizer\n","\n","nb_words = len(word2idx_inputs) # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word2idx_inputs.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        \n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))\n","print (\"Shape de los embeddings: \",embedding_matrix.shape )\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 4 - Entrenar el modelo\n","Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_6 (InputLayer)        [(None, 9)]                  0         []                            \n","                                                                                                  \n"," input_7 (InputLayer)        [(None, 10)]                 0         []                            \n","                                                                                                  \n"," embedding_2 (Embedding)     (None, 9, 50)                89950     ['input_6[0][0]']             \n","                                                                                                  \n"," embedding_3 (Embedding)     (None, 10, 128)              231296    ['input_7[0][0]']             \n","                                                                                                  \n"," lstm_2 (LSTM)               [(None, 128),                91648     ['embedding_2[0][0]']         \n","                              (None, 128),                                                        \n","                              (None, 128)]                                                        \n","                                                                                                  \n"," lstm_3 (LSTM)               [(None, 10, 128),            131584    ['embedding_3[0][0]',         \n","                              (None, 128),                           'lstm_2[0][1]',              \n","                              (None, 128)]                           'lstm_2[0][2]']              \n","                                                                                                  \n"," dense_1 (Dense)             (None, 10, 1807)             233103    ['lstm_3[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 777581 (2.97 MB)\n","Trainable params: 687631 (2.62 MB)\n","Non-trainable params: 89950 (351.37 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","\n","n_units = 128\n","\n","# define training encoder\n","encoder_inputs = Input(shape=(max_input_len))\n","\n","#encoder_embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)\n","\n","encoder_embedding_layer = Embedding(\n","          input_dim=nb_words,  # definido en el Tokenizador\n","          output_dim=embed_dim,  # dimensión de los embeddings utilizados\n","          input_length=max_input_len, # tamaño máximo de la secuencia de entrada\n","          weights=[embedding_matrix],  # matrix de embeddings\n","          trainable=False)      # marcar como layer no entrenable\n","\n","encoder_inputs_x = encoder_embedding_layer(encoder_inputs)\n","\n","encoder = LSTM(n_units, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs_x)\n","encoder_states = [state_h, state_c]\n","\n","# define training decoder\n","decoder_inputs = Input(shape=(max_out_len))\n","decoder_embedding_layer = Embedding(input_dim=num_words_output, output_dim=n_units, input_length=max_out_len)\n","decoder_inputs_x = decoder_embedding_layer(decoder_inputs)\n","\n","decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n","\n","# Dense\n","decoder_dense = Dense(num_words_output, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}],"source":["# Modelo completo (encoder+decoder) para poder entrenar\n","from tensorflow.keras.utils import plot_model\n","\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["encoder_model = Model(encoder_inputs, encoder_states)\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}],"source":["from tensorflow.keras.utils import plot_model\n","plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# Modelo solo decoder (para realizar inferencia)\n","# from tensorflow.keras.utils import plot_model\n","\n","# define inference decoder\n","decoder_state_input_h = Input(shape=(n_units,))\n","decoder_state_input_c = Input(shape=(n_units,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# En cada predicción habrá una sola palabra de entrada al decoder,\n","# que es la realimentación de la palabra anterior\n","# por lo que hay que modificar el input shape de la layer de Embedding\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding_layer(decoder_inputs_single)\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","# plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","150/151 [============================>.] - ETA: 0s - loss: 3.0220 - accuracy: 0.5322"]},{"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node model_3/embedding_2/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_868\\1146845154.py\", line 1, in <module>\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1832, in fit\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2272, in evaluate\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 4079, in run_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2042, in test_function\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2025, in step_function\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2013, in run_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1893, in test_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 589, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[14,8] = 1799 is not in [0, 1799)\n\t [[{{node model_3/embedding_2/embedding_lookup}}]] [Op:__inference_test_function_17523]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\clase_6\\ejercicios\\6- bot_qa.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     [encoder_input_sequences, decoder_input_sequences],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     decoder_targets,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model_3/embedding_2/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\santi\\AppData\\Local\\Temp\\ipykernel_868\\1146845154.py\", line 1, in <module>\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1832, in fit\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2272, in evaluate\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 4079, in run_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2042, in test_function\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2025, in step_function\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2013, in run_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1893, in test_step\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 589, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[14,8] = 1799 is not in [0, 1799)\n\t [[{{node model_3/embedding_2/embedding_lookup}}]] [Op:__inference_test_function_17523]"]}],"source":["hist = model.fit(\n","    [encoder_input_sequences, decoder_input_sequences],\n","    decoder_targets,\n","    epochs=50, \n","    validation_split=0.2)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'hist' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\santi\\IA\\4 Bimestre\\procesamiento_lenguaje_natural\\clase_6\\ejercicios\\6- bot_qa.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Entrenamiento\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m epoch_count \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(hist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sns\u001b[39m.\u001b[39mlineplot(x\u001b[39m=\u001b[39mepoch_count,  y\u001b[39m=\u001b[39mhist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/santi/IA/4%20Bimestre/procesamiento_lenguaje_natural/clase_6/ejercicios/6-%20bot_qa.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sns\u001b[39m.\u001b[39mlineplot(x\u001b[39m=\u001b[39mepoch_count,  y\u001b[39m=\u001b[39mhist\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"]}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Entrenamiento\n","epoch_count = range(1, len(hist.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate([encoder_input_sequences, decoder_input_sequences], decoder_targets)"]},{"cell_type":"markdown","metadata":{"id":"Zbwn0ekDy_s2"},"source":["### 5 - Inferencia\n","Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Armar los conversores de índice a palabra:\n","idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n","idx2word_target = {v:k for k, v in word2idx_outputs.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def answer_question(input_seq):\n","    # Se transforma la sequencia de entrada a los estados \"h\" y \"c\" de la LSTM\n","    # para enviar la primera vez al decoder\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = word2idx_outputs['<sos>']\n","\n","    # Se obtiene el índice que finaliza la inferencia\n","    eos = word2idx_outputs['<eos>']\n","    \n","    output_sentence = []\n","    for _ in range(max_out_len):\n","        # Predicción del próximo elemento\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","        idx = np.argmax(output_tokens[0, 0, :])\n","\n","        # Si es \"end of sentece <eos>\" se acaba\n","        if eos == idx:\n","            break\n","\n","        # Transformar idx a palabra\n","        word = ''        \n","        if idx > 0:\n","            word = idx2word_target[idx]\n","            output_sentence.append(word)\n","\n","        # Actualizar los estados dada la última predicción\n","        states_value = [h, c]\n","\n","        # Actualizar secuencia de entrada con la salida (re-alimentación)\n","        target_seq[0, 0] = idx\n","\n","    return ' '.join(output_sentence)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
